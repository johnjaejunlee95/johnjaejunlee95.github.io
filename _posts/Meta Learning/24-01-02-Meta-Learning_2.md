---

layout: post
gh-repo: johnjaejunlee95/johnjaejunlee95.github.io
gh-badge: [star, follow]
comments: true
author: johnjaejunlee95
title: "[ê°œë…ì„¤ëª…] Meta Learning (2) - Approaches"
date: "2024-03-04"
description: ""
categories: [Meta Learning]
toc: false
toc_sticky: true
tags: [Meta-Learning, Few-Shot Learning, MAML, Reptile, MatchingNet, ProtoNet]
use_math: true
author_profile: true
published: true
sidebar:
  nav: "docs"
---

<div>ì´ì „ postingì—ì„œëŠ” meta-learningì´ ë‚˜ì˜¤ê²Œ ëœ ë§¥ë½, ê·¸ë¦¬ê³  meta-learningì„ ì´í•´í•˜ê¸° ìœ„í•¸ ê¸°ë³¸ ê°œë…ì¸ few-shot learningì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ postingì—ì„œëŠ” meta-learning approachesë“¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì‹œì´ˆê°€ ëœ ë…¼ë¬¸ë“¤ì´ ë¬´ì—‡ì¸ì§€, ê·¸ë¦¬ê³  ê° ë…¼ë¬¸ë“¤ì—ì„œ ì–˜ê¸°í•´ê³  ì‹¶ì€ pointê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ì •ë¦¬í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. 
<br><br>
  ë‹¤ë§Œ... 2017ë…„ë¶€í„° í•´ì„œ ë…¼ë¬¸ë“¤ì´ ë§¤ìš° ë§ì´ ë‚˜ì™”ê¸° ë•Œë¬¸ì— ëª¨ë“  ë…¼ë¬¸ë“¤ì„ ë‹¤ë£¨ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, í•µì‹¬ì´ ë˜ëŠ” ë…¼ë¬¸ë“¤, ë˜ ì œê°€ ì¬ë°Œê²Œ ì½ì—ˆë˜ ë…¼ë¬¸ ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ë ¤í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‹¤ìŒ postingì—ì„œëŠ” advanced methodsë“¤ ìœ„ì£¼ì˜ ë…¼ë¬¸ë“¤ì„ ê°„ëµí•˜ê²Œ ë¦¬ë·°í•˜ëŠ” ì‹œê°„ì„ ê°–ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ :smiley: </div>


## 2. Meta Learning Apporaches

ì‚¬ì‹¤, ì´ì „ postì—ì„œ few-shot learningì„ ì„¤ëª…í•œ ì´ìœ ëŠ” meta learningì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œì˜€ìŠµë‹ˆë‹¤. Few-shot learningì˜ ê°œë…ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•˜ê²Œ approachë¥¼ ì ìš©í•˜ëŠ”ê²Œ meta learningì´ë¼ê³  ì´í•´í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ê·¸ëŸ¼ ê±°ë‘ì ˆë¯¸í•˜ê³  meta learningì—ëŠ” ì–´ë–¤ ê°œë…ë“¤ì´ ìˆëŠ”ì§€ ë³´ê² ìŠµë‹ˆë‹¤.

Meta learningì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í¬ê²Œ 3ê°€ì§€ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Optimization-based Approach
- Metric-based Approach
- Model-based Approach

ê° approachë“¤ì— ëŒ€í•´ì„œ ì¤‘ìš” ë…¼ë¬¸ë“¤ì˜ í•µì‹¬ approach ìœ„ì£¼ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì—¬ê¸°ì„œëŠ” model-based approachëŠ” skipí•˜ê² ìŠµë‹ˆë‹¤. model-based approachëŠ” ë³´í†µ RLì—ì„œ ë§ì´ ì“°ì´ê¸° ë•Œë¬¸ì— ë‹¤ìŒì— ê¸°íšŒê°€ ëœë‹¤ë©´ ë”°ë¡œ posting í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!!

> $S$ ì™€ $Q$ë¥¼ ì–´ë–»ê²Œ í•™ìŠµì— í™œìš©í•˜ëŠ”ì§€ë¥¼ ì²´í¬í•˜ë©´ì„œ ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 



### 2.1 Optimization-based Meta Learning

Optimization-based meta learningì€ gradientë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ì— ë”°ë¼ ì œì¼ ë¨¼ì € ì–¸ê¸‰ë˜ëŠ” ë…¼ë¬¸ì€ ë°”ë¡œ 2017ë…„ì— ë‚˜ì˜¨ [Model-Agnostic Meta-Learning (MAML)](https://arxiv.org/pdf/1703.03400.pdf) ì…ë‹ˆë‹¤. ì‚¬ì‹¤ìƒ "Meta Learning"ì´ë¼ëŠ” ê°œë…ì„ ëŒ€ì¤‘í™”ì‹œí‚¨ ë…¼ë¬¸ì´ì£ . ê·¸ë ‡ë‹¤ë©´ MAMLì€ ì–´ë–¤ ë…¼ë¬¸ì¸ì§€ í•œë²ˆ ì‚´í´ë´…ì‹œë‹¤.

#### 2.1.1 MAML

MAMLì˜ ìµœì¢… ëª©ì ì€ fast adaptation/finetuning (ì´í•˜ FT) ì´ ê°€ëŠ¥í•œ ìœ„ì¹˜ë¡œ model parameter $\theta$ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼í•  keywordsëŠ” "FT"ì…ë‹ˆë‹¤. MAMLì˜ í•µì‹¬ ê¸°ì‘ì€ ê²°êµ­ "ëª‡ stepì˜ parameter updateë¥¼ í†µí•´ì„œ íŠ¹ì • taskë“¤ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤ë©´ íš¨ìœ¨ì ì¼ ê²ƒì´ë‹¤!" ì…ë‹ˆë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ë³´ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ìš°ë¦¬ë‚˜ë¼ ì „ ì§€ì—­ì„ ëŒì•„ë‹¤ë…€ì•¼í•˜ëŠ” ì§ì—…ì„ ê°€ì§„ ì‚¬ëŒì´ë¼ë©´ ë¶€ì‚°, ê°•ë¦‰, ì¸ì²œ ë“±ì—ì„œ ê±°ì£¼í•˜ëŠ” ê²ƒë³´ë‹¤ ëŒ€ì „ì—ì„œ ê±°ì£¼í•˜ëŠ”ê²Œ íš¨ìœ¨ì ì¼ ê²ƒì…ë‹ˆë‹¤. ì´ì²˜ëŸ¼, ëª¨ë“  taskë¥¼ ì¼ì¼ì´ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ taskì— ë¹ ë¥´ê²Œ ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜ë¡œ $\theta$ë¥¼ ì˜®ê²¨ë†“ëŠ”ê²Œ ë‹¤ì–‘í•œ taskì— ëŒ€í•´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ìœ„ ë¬¸ë‹¨ì—ì„  êµ¬êµ¬ì ˆì ˆ ë§ë¡œ ì„¤ëª…í–ˆë‹¤ë©´ ì´ë²ˆì—” ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ê² ìŠµë‹ˆë‹¤. ë“¤ì–´ê°€ê¸°ì— ì•ì„œ MAMLì€ í•œ epochì„ í•™ìŠµí•˜ëŠ”ë° inner-loop, outer-loopë¡œ ë‚˜ë‰˜ì–´ì§‘ë‹ˆë‹¤. inner-loopë•ŒëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í•œ FT, outer-loopëŠ” model update$^*$ì…ë‹ˆë‹¤. MAML algorithmì€ ë‹¤ìŒ <a href='#figure1'>Figure 1</a>ì™€ ê°™ìŠµë‹ˆë‹¤. 

$^*$ ë³´í†µ ì´ ê³¼ì •ì„ bi-level optimizationì´ë¼ê³ ë„ í•©ë‹ˆë‹¤.

![image.png1](/images/23-03-13/MAML_Diagram.png) |![image.png2](/images/23-03-13/MAML_algo.png)

<center>
  <figcaption>
    <a id='figure1'>Figure 1. Diagram and Algorithm of MAML </a>
  </figcaption>
</center>

ê·¸ë¦¬ê³  ìœ„ pseudo-codeë¥¼ í’€ì–´ì„œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. Initialize model parameter $\theta$ (line 1)
2. Sample task $\mathcal{T}$ =($S$,$\mathcal{Q}$ ) ; $\mathcal{T} \sim p(\mathcal{T})$ along with the number of batches (line 3)
3. Inner-Loop updates with $n$ steps (line 4-7):  
   1. Repeat SGD update: $\phi = \theta - \alpha\nabla_\theta \mathcal{L}(S;\theta)$ 
   2. $n \geq 2$ë¶€í„°ëŠ” $\theta$ â†’ $\phi$ ìœ¼ë¡œ ë°”ë€œ; ì¦‰ $\phi = \phi - \alpha \nabla_\phi \mathcal{L}(\mathcal{S}; \phi)$ ê°™ì€ í˜•íƒœë¡œ update
4. Outer-Loop (line 8): 
   1. With fine-tuned model $\phi$, evaluate with $\mathcal{Q}$ and update
   2. $\Rightarrow$ $\theta \leftarrow \theta - \frac{1}{N}\sum_{i=1}^N \nabla_\theta \mathcal{L}_i(\mathcal{Q};\phi) $

5. Repeat 2-4 (line 2-9)

ì—¬ê¸°ì„œ ë˜ ëˆˆì—¬ê²¨ë³´ì•„ì•¼ í•  ë¶€ë¶„ì€ outer loop ë•Œì˜ derivative(meta-gradient) ì…ë‹ˆë‹¤. ë³´ë©´ outer loop updateë¥¼ í•  ë•Œ fine-tunedëœ parameter $\phi$ì™€ $Q$ë¡œ ì¸í•´ ë‚˜ì˜¨ lossê°’ì„ $\phi$ë¡œ ë¯¸ë¶„í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ $\theta$ìœ¼ë¡œ ë¯¸ë¶„ì„ í•©ë‹ˆë‹¤. ê´€ë ¨ ìˆ˜ì‹ì€ chain ruleì— ì˜í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œë©ë‹ˆë‹¤.


$$
\begin{aligned} 
\nabla_\theta \mathcal{L}(\mathcal{Q};\phi) &= \frac{\partial }{\partial \theta}\mathcal{L}(\mathcal{Q};\phi) \\
&= \frac{\partial \mathcal{L}(\mathcal{Q};\phi)}{\partial \phi} \frac{\partial \phi}{\partial \theta} \\
&= \nabla_\phi \mathcal{L}(\mathcal{Q};\phi)\nabla_\theta \phi \\ 
&=\nabla_\phi \mathcal{L}(\mathcal{Q};\phi)\nabla_\theta (\theta - \alpha \nabla_\theta \mathcal{L}(\mathcal{S};\theta)) \\
&= \nabla_\phi \mathcal{L}(\mathcal{Q};\phi)(I - \alpha \nabla^2_\theta \mathcal{L}(\mathcal{S};\theta)) \\
\end{aligned}
$$

ì´ ìˆ˜ì‹ì˜ ëª©ì ì€ ê²°êµ­ ìµœì¢…ì ìœ¼ë¡œ $\theta$ë¥¼ lossê°€ ë‚®ì€ ìª½ìœ¼ë¡œ update í•˜ìëŠ” ê²ƒì¸ë°, ê·¸ ë°©í–¥ì„ FTëœ $\phi$ì—ì„œ loss $\mathcal{L}(\mathcal{Q};\phi)$ë¥¼ ë‚®ì¶”ëŠ” ì§€ì ìœ¼ë¡œ updateí•˜ìëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë§ì´ ëª¨í˜¸í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆëŠ”ë°, <a href='#figure2'>Figure 2</a>ë¥¼ ë³´ì‹œë©´ update ë°©í–¥ì— ëŒ€í•´ ì–´ëŠì •ë„ ì´í•´ê°€ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. (Reference: [Boyang Zhao's Blog](https://boyangzhao.github.io/posts/few_shot_learning), notationì´ ì‚´ì§ ë‹¤ë¥¸ë° lossëŠ” ê·¸ëƒ¥ lossë¡œ ë´ì£¼ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤!!)

![](/images/23-12-24/maml_task.png)|![](/images/23-12-24/maml_task_multi.png)

<center>
  <figcaption>
    <a id='figure2'>Figure 2. Visaulization of how MAML updates $\theta$; $\mathcal{D}^{tr}= S, \mathcal{D}^{ts}=Q$Â </a>
  </figcaption>
</center>



#### 2.1.2 FOMAML, Reptile

MAML ê°™ì€ ê²½ìš°, hessian matrix multiplication($=\nabla_\theta^2 \mathcal{L}(\mathcal{S};\phi)$)ì´ ë“¤ì–´ê°€ ìˆì–´ computational costì ì¸ ê´€ì ì—ì„œ penaltyê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì„±ëŠ¥ì„ ì–´ëŠì •ë„ ìœ ì§€í•˜ë©´ì„œ computational costë¥¼ ì¤„ì´ëŠ” ë°©ë²•ë“¤ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

ê·¸ ì¤‘ í•˜ë‚˜ê°€ FOMAML (First-Order MAML) ì…ë‹ˆë‹¤. FOMAMLì€ MAML ë…¼ë¬¸ì—ì„œ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•œ ê²ƒìœ¼ë¡œ, hessian matrixë¥¼ ë¬´ì‹œí•œ ì±„ í•™ìŠµì„ ì§„í–‰í•´ë„ ì–´ëŠì •ë„ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰ $\nabla_\theta^2 \mathcal{L}(\mathcal{S};\phi) = 0$ ì´ë¼ê³  ê°€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê´€ë ¨í•´ì„œ <a href='#figure3'>Figure 3</a>ì— ì˜ ë‚˜íƒ€ë‚˜ ìˆëŠ”ë°, fintuningëœ $\phi$ ì—ì„œ loss $\mathcal{L}(\mathcal{Q};\phi)$ë¥¼ ë‚®ì¶”ëŠ” "gradientì˜ ë°©í–¥"ì„  $\theta$ ì— ì ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ ê¸°ì‘ì´ ê°€ëŠ¥í•œ ì´ìœ ë¥¼ ReLUë¥¼ ê±°ì¹˜ë©´ì„œ hessian ê°’ì´ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. Loss landscape ê´€ì ì—ì„œ ìƒê°ì„ í•´ë³´ë©´, "lossë¥¼ ë‚®ì¶”ëŠ” ë°©í–¥"ì´ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, ìµœì¢… updateëœ MAMLì—ì„œì˜ $\theta$ ìœ„ì¹˜ì™€ FOMAMLì—ì„œì˜ $\theta$ ìœ„ì¹˜ê°€ ë¹„ìŠ·í•œ loss landscapeì— ìˆë‹¤ëŠ” ê°€ì •ì´ ì•”ë¬µì ìœ¼ë¡œ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì´ì£ .



<center>
  <img width="70%" height="70%" src="/images/23-12-24/fomaml_task.png"> <br>
  <br>
  <figcaption>
    <a id='figure3'>Figure 3. Visualization of how FOMAML updates $\theta$</a>
  </figcaption>
  <br>
</center>
ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” [Reptile](https://arxiv.org/pdf/1803.02999.pdf) ë…¼ë¬¸ì…ë‹ˆë‹¤. Reptile ë…¼ë¬¸ì€ 2018ë…„ì— OpenAIì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ìœ¼ë¡œ, FOMAMLì˜ variant ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. Reptileì˜ íŠ¹ì§•ì€ $S$ì™€ $Q$ê°€ ë”°ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Few-shotìœ¼ë¡œ í•™ìŠµí•˜ì§€ë§Œ, taskë¥¼ ì—¬ëŸ¬ê°œ ë½‘ì•„ë‘ê³  samplingì„ í†µí•´ì„œ ë½‘ì€ taskë¥¼ í•™ìŠµì„ ì‹œí‚µë‹ˆë‹¤. ê·¸ë¦¬ê³  initial model parameter $\theta$ì™€  fine-tunedëœ model parameter $\phi$  ì°¨ì´ë¥¼ gradient ì‚¼ì•„ updateì„ ì§„í–‰í•©ë‹ˆë‹¤. ì•„ë˜ì— <a href="#figure4">FigureÂ 4</a>ëŠ” Reptile algorithmê³¼ update ë°©í–¥ì— ëŒ€í•œ ê·¸ë¦¼ì…ë‹ˆë‹¤.

<img src="/images/23-12-24/reptile.png">|<img src="/images/23-12-24/reptile_task.png" style="zoom:140%;">

<center>
  <figcaption>
    <a href="figure4">Figure 4. Overview of Reptile Algorithm and Visualization of how Reptile updates $\theta$</a>
  </figcaption>
  <br>
</center>



ê¸°ì¡´ MAML ë…¼ë¬¸ì— ë‚˜ì˜¨ notationê³¼ ë‹¬ë¼ì„œ í—·ê°ˆë¦´ìˆ˜ë„ ìˆëŠ”ë°, processë¥¼ í’€ì–´ì„œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. Initialize model parameter $\theta$ 
2. Task $\mathcal{T}_i$ ë¥¼ $N$ê°œ ë½‘ëŠ”ë‹¤. (where $\mathcal{T}_i \sim p(\mathcal{T})$, batch = $N$)
3. Inner Loop: ê° task $\mathcal{T}_i$ë³„ë¡œ FT (fine-tuned parameters: $\phi_i$)
4. Outer Loop: $\theta$ì™€ $\phi$ì˜ ì°¨ì´ë§Œí¼ $\theta$ update: $\theta \leftarrow \theta + \frac{\beta}{N}\sum_{i=1}^N (\phi_i - \theta)$
5. repeat 2-4

ì´ì™€ ê°™ì´ í•™ìŠµí–ˆì„ ë•Œ ê²°êµ­ Reptile algorithmì—ì„œ meta-gradientì˜ expectation ê°’ì´ MAMLì˜ meta-gradientì™€ ë¹„ìŠ·í•˜ê²Œ ìˆ˜ë ´í•˜ê²Œ ë©ë‹ˆë‹¤. Reptile ë…¼ë¬¸ì˜ ëŒ€ë¶€ë¶„ì´ ìˆ˜í•™ì ìœ¼ë¡œ MAML, FOMAMLê³¼ ì–´ë–»ê²Œ ë¹„ìŠ·í•˜ê²Œ ìˆ˜ë ´í•˜ëŠ”ì§€ë¥¼ ì¦ëª…í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. ë‹¤ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ëµí•˜ê²Œ í° ë§¥ë½ì—ì„œë§Œ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì´ ì—­ì‹œ ì–¸ì  ê°€ ê¸°íšŒê°€ ëœë‹¤ë©´ ì¦ëª… ê´€ë ¨ëœ postingì„ í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

#### 2.1.3 Wrap-up

Optimization-based Meta Learningì€ few-shot learningì„ í•  ë•Œ gradientë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ì—¬ í•™ìŠµí• ì§€ì— ëŒ€í•œ meta learning approach ì…ë‹ˆë‹¤. ë‹¤ë¥¸ approachë“¤ê³¼ëŠ” ë‹¬ë¦¬, gradientë§Œ ì ì ˆí•˜ê²Œ ì„¤ì •í•œë‹¤ë©´ ë‹¤ì–‘í•œ ë¶„ì•¼ì— model-agnosticí•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì¦‰, Regression, classification, reinforcement learning ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### 2.2 Metric-based Meta Learning

Metric-based meta learningì€ ë§ ê·¸ëŒ€ë¡œ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ similarityë¥¼ ê³„ì‚°í•´ í•™ìŠµí•˜ëŠ” ê°œë…ì…ë‹ˆë‹¤. ì‰½ê²Œ ì–˜ê¸°í•´ ê° class ë³„ë¡œ ê°€ì§€ê³  ìˆëŠ” sementic ì •ë³´ê°€ ìˆì„í…ë°, ê·¸ sementic ì •ë³´ê°„ì˜ similarityë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ì–´ë–»ê²Œ ë³´ë©´ $k$-NN ë“±ê³¼ ê°™ì€ nearest neighborsì˜ ê°œë…ê³¼ ë¹„ìŠ·í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œëŠ” Matching Network, Prototypical Network, Relation Networkê°€ ìˆëŠ”ë°, ê° ë…¼ë¬¸ë“¤ì—ì„œ ì–˜ê¸°í•˜ëŠ” ì£¼ìš” ê°œë…ë“¤ì„ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. (Few-shotì— ëŒ€í•œ ê°œë…ì„ ê³„ì† ì—¼ë‘í•´ë‘ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!!)

#### 2.2.1 Matching Network

ì²˜ìŒìœ¼ë¡œ ë³¼ ë…¼ë¬¸ì€ [Matching Network](https://arxiv.org/pdf/1606.04080.pdf)ì…ë‹ˆë‹¤. Metric-based approachì—ì„œì˜ ì‹œì´ˆê²©ì¸ ë…¼ë¬¸ì…ë‹ˆë‹¤. ê·¸ ë‹¹ì‹œì˜ ìƒí™©ì„ ìƒê°í•´ë³´ë©´, Transformerì˜ ê·¼ê°„ì´ ëœ seq2seq ë…¼ë¬¸ì´ ë‚˜ì™”ìŠµë‹ˆë‹¤. Attention mechanismì„ í†µí•´ extracted featureë“¤ë§Œ ë³´ëŠ” ê²ƒì´ ì•„ë‹Œ ì „ì²´ì ì¸ ë§¥ë½, ì¦‰ contextë¥¼ í†µí•´ í•™ìŠµì„ í•˜ìëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ ë…¼ë¬¸ì—ì„œëŠ” encoderë¥¼ í†µí•´ ë‚˜ì˜¨ featureê°„ì˜ contextë¥¼ ë¹„êµë¥¼ í•˜ê² ë‹¤ëŠ” ê²ë‹ˆë‹¤. 

<center>
  <img src="/images/23-12-24/matching.png" width="60%" height="60%">
  <figcaption>
    <a href="figure5">Figure 5. Overview of Matching Network Algorithm</a>
  </figcaption>
</center>
ìš°ì„  í•™ìŠµ processë¥¼ ì„¤ëª…í•˜ê¸°ì— ì•ì„œ, ì´ ë…¼ë¬¸ì—ì„œëŠ” ìµœì¢… outputì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤. 


$$
C_{\mathcal{S}}(\hat{\textbf{x}}) = P(y|\hat{\textbf{x}}, S) = \sum_{i=1}^k a(\hat{\textbf{x}}, \textbf{x}_i)y_i \; \;\; \text{where} \; \mathcal{S=\{ \text{(}\textbf{x}_i, y_i\text{)}\}_{i=1}^{k}}
$$


ìœ„ ìˆ˜ì‹ì€ attention mechanismì„ ê°€ì¤‘ì¹˜ë¥¼ ë” ì£¼ëŠ” ê²ƒìœ¼ë¡œ í™œìš©ì„ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ $a(\cdot, \cdot)$â€‹ì€ attention kernelë¡œ cosine similarityì— softmaxë¥¼ ì·¨í•©ë‹ˆë‹¤:


$$
a(\hat{x},x) =\frac{\exp(cos(f(\hat{x}), g(x)))}{\sum_{j=1}^k \exp(cos(f(\hat{x}), g(x_j)))}
$$


ìœ„ì˜ ì˜ë¯¸ë¥¼ ë‹¨ìˆœí•˜ê²Œ í•´ì„í•´ë³´ë©´, ê° support sample $\textbf{x}_i $ì™€ input $\textbf{x}$â€‹ì— ëŒ€í•´ì„œ attention kernelì„ í†µí•´ì„œ similarityë¥¼ ê³„ì‚° í›„ ì´ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í™œìš©í•˜ì—¬ ë¹„êµ ì‹œ similarityê°€ ë” ë†’ì€ ìª½ìœ¼ë¡œ predictí•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ input $\textbf{x}$ëŠ” query sampleë¡œ ì´í•´í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤. 

ê·¸ë ‡ë‹¤ë©´ notationê³¼ í•¨ê»˜ í•™ìŠµ processë¥¼ ë³´ì‹œê² ìŠµë‹ˆë‹¤.

1. $g_\theta$â€‹ ë¥¼ í†µí•´ support setì˜ feature representation vector ë½‘ê¸°
2. $f_\theta$ ë¥¼ í†µí•´ query setì˜ feature representation vector ë½‘ê¸°  (ë³´í†µ $f_\theta = g_\theta$ )
3. 1ê³¼ 2ì—ì„œ ë‚˜ì˜¨ feature representation vectorë¼ë¦¬ attention ê°’ êµ¬í•˜ê¸° $\rightarrow  a(\cdot, \cdot)$
4. $C_\mathcal{S}$ ì„ í†µí•´ query setì˜ label predict í•˜ê¸°

ìµœê·¼ ë…¼ë¬¸ë“¤ê³¼ ë‹¬ë¦¬, meta learning ê´€ë ¨ ì´ˆê¸° ë…¼ë¬¸ë“¤ì—ì„œëŠ” few-shot settingì„ context ê´€ì ì—ì„œ í•´ê²°í•˜ê¸° ìœ„í•´ LSTM êµ¬ì¡°ë¥¼ í™œìš©í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œë„ LSTM êµ¬ì¡°ë¥¼ í™œìš©í•œ ë°©ë²•ì„ ì¶”ê°€ë¡œ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ($\rightarrow$ Full Context Embeddings; FCE) LSTM model architectureë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€  notationê³¼ í•¨ê»˜ FCE í•™ìŠµ processë¥¼ ë³´ê² ìŠµë‹ˆë‹¤.

- Embedding $g$
  - $g \rightarrow $ bidirectional LSTM, $g' \rightarrow$â€‹â€‹â€‹â€‹ CNN (feature extractor) 
  -  $g(x_i, \mathcal{S})= \overrightarrow{h}\_i + \overleftarrow{h}\_i + g^\prime (x_i) $ 
  - $\overrightarrow{h}_i,\overrightarrow{c}\_i = \text{LSTM}(g^{\prime} (x_i), {\overrightarrow{h}}\_{i-1},  {\overrightarrow{c}}\_{i-1})$ ,  $\overleftarrow{h}_i,\overleftarrow{c}\_i = \text{LSTM}(g^{\prime} (x_i), {\overleftarrow{h}}\_{i+1},  {\overleftarrow{c}}\_{i+1})$
- $f \rightarrow$ LSTM , $f' \rightarrow$ CNN (feature extractor) 
  - $f(\hat{x}, \mathcal{S}) = \text{attLSTM}(f^\prime(\hat{x}), g(\mathcal{S}), K) $

$\Rightarrow k$ stepì— ë”°ë¼...

1. $\hat{h}\_k,  c_k = \text{LSTM}(f^\prime (\hat{x}), [h\_{k-1}, r\_{k-1}], c\_{k-1}) $â€‹
2. $h_k = \hat{h}_k + f^\prime(\hat{x})$
3. $r\_{k-1} = \sum\_{i=1}^{\|\mathcal{S}\|}a(h\_{k-1}, g(x\_i))g(x\_i)$
4. $a(h\_{k-1}, g(x\_i)) = \text{softmax}(h^\text{T}\_{k-1}g(x_i))$

ê²°êµ­ ì—¬ê¸°ì„œ LSTMì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ê° feature vectorë“¤ì˜ contextë¥¼ ë” ì˜ ë³´ê¸° ìœ„í•¨ì¸ë°, Omniglot ê°™ì€ ì‰¬ìš´ taskì—ì„œëŠ” ì„±ëŠ¥ gainì´ ë³„ë¡œ ì—†ì§€ë§Œ $mini$-ImageNet ê°™ì€ ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ taskì—ì„œëŠ” ì„±ëŠ¥ gainì´ ìˆìŠµë‹ˆë‹¤.



#### 2.2.2 Prototypical Networks

ê·¸ ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œëŠ” [Prototypical Networks](https://arxiv.org/pdf/1703.05175.pdf) (ì´í•˜ ProtoNet)ì…ë‹ˆë‹¤. ì‚¬ì‹¤, metric-based meta learning ì—°êµ¬ëŠ” ëŒ€ë¶€ë¶„ matching networkë³´ë‹¤ ProtoNetì„ ê¸°ë°˜ìœ¼ë¡œ ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. 

ë°”ë¡œ ë³¸ë¡ ìœ¼ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤. ProtoNetì€ ê° labelì˜ prototype vectorì™€ feature vectorë“¤ ê°„ì˜ euclidean distance ì—°ì‚°ì„ í†µí•´ì„œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì•„ë˜ <a href="#figure6">Figure 6</a>ë¥¼ ë³´ì‹œë©´, $c_n$ë“¤ì´ ê° labelì˜ prototypeì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ìƒˆë¡œìš´ taskì— ëŒ€í•´ì„œ ê° prototypeê³¼ì˜ distanceë¥¼ êµ¬í•˜ê³  minimum distanceì¸ prototype labelë¡œ mapping ì‹œí‚µë‹ˆë‹¤. ì´ ë•Œ prototypeì€ support setì„ í†µí•´ ë‚˜ì˜¨ feature vectorì˜ í‰ê· ìœ¼ë¡œ êµ¬í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒ í•™ìŠµ processë¥¼ í†µí•´ì„œ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

- Notation (ìµœëŒ€í•œ ë…¼ë¬¸ê³¼ ìœ ì‚¬í•˜ê²Œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤):
  - Support Set $\mathcal{S}\_{n}= \\{ (x\_{n,j}^s, y\_{n,j}^s) \\}\_{j=1}^{K}$,  Query Set $\mathcal{Q}\_{n}= \\{ (x\_{n,j}^q, y\_{n,j}^q) \\}\_{j=1}^{Q}$â€‹ 
  - $K$: support set ê°¯ìˆ˜ (a.k.a $K$-shot)
  - $Q$: query set ê°¯ìˆ˜ 
  - $c_n$: label $n$ì˜ prototype $\rightarrow$ $\\{c_1,\dots,c_N \\}$, ($N$: $N$-way)
  - $f_\theta$ : Model parameterized by $\theta$â€‹ (ì´í•˜ feature extractor or backbone network)
  - loss $\mathcal{L}(\mathcal{D},c,\theta) = \frac{1}{\|\mathcal{D}\|}\sum\_{(x,y)\in \mathcal{D}} l(-d(f_\theta(x), c),y)$, \\
    $\rightarrow$ loss function $l(\cdot, \cdot)$: Cross Entropy (CE),  $-d(\cdot, \cdot)$: Euclidean Distance

1. $c_n = \frac{1}{\| \mathcal{S}\_n \|} \sum\_{j=1}^{\| \mathcal{S}\_n \|} f\_\theta (x^s\_{n, j}) \Rightarrow $ **<mark>support set</mark>**ìœ¼ë¡œ **<mark>prototype vector ${c_n}$</mark>**êµ¬í•˜ê¸°  
2. $\sum\_{n=1}^{N}\mathcal{L}(Q\_{n}, c_n, \theta) \Rightarrow$ **<mark>query set</mark>**ê³¼ **<mark>prototype $c_n$â€‹</mark>**ê°„ **<mark>euclidean distance</mark>** êµ¬í•˜ê¸°   
3. $\theta \leftarrow \theta - \nabla\_{\theta}\sum\_{n=1}^{N}\mathcal{L}(Q\_{n}, c_n, \theta)$ $\Rightarrow$ **<mark>model parameter update</mark>**



ìƒë‹¹íˆ notationì´ ë§ê³  ë³µì¡í•´ì„œ ì´í•´í•˜ê¸° ì–´ë ¤ìš°ì‹¤ ìˆ˜ ìˆì„ê±°ë¼ ìƒê°í•©ë‹ˆë‹¤. ë§Œì•½ ì¡°ê¸ˆ ê³¼ì •ì´ ë³µì¡í•˜ì‹œë‹¤ë©´ ê·¸ëƒ¥ ë‹¨ìˆœí•˜ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ì´í•´í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.



- Support Setìœ¼ë¡œ  prototypeì´ë¼ëŠ” label ë§Œë“¤ê¸°
- Query Setìœ¼ë¡œ prototypeê°„ì˜ ê±°ë¦¬ ë¹„êµí•˜ê¸° $\Rightarrow$ Logits (ìµœì¢… output)
- Query Set Labelê³¼ Logitsê°„ CE êµ¬í•˜ê¸°
- CEë¡œ parameter updateí•˜ê¸°



<center>
  <img src="/images/23-12-24/protonet.png" width="100%" height="100%">
  <figcaption>
    <a href="figure6">Figure 6. Overview of ProtoNet</a>
  </figcaption>
</center>



ì´ ë…¼ë¬¸ì—ì„œëŠ” linear layerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì–´ì°¨í”¼ feature vectorë¥¼ ê±°ë¦¬ êµ¬í•˜ëŠ”ë° ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— classification taskì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì´ ë…¼ë¬¸ì—ì„œëŠ” euclidean distanceë¥¼ í•˜ë‚˜ì˜ linear modelì²˜ëŸ¼ reinterpretationì´ ê°€ëŠ¥í•˜ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ë‹¤ìŒ ë‘ê°œì˜ ìˆ˜ì‹ì„ ë³´ë©´ì„œ ì„¤ëª…ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


$$
-||f_\theta (x) - c_k||^2 = -f_\theta (x)^{\text{T}}\cdot f_\theta(x) +2c_k^{\text{T}}\cdot f_\theta(x) -c_k^{\text{T}}\cdot c_k \\
$$

$$
2c_k^{\text{T}} \cdot f_\theta (x) - c_k^{\text{T}} \cdot c_k = w_k^{\text{T}}f_\theta(x) +b_k \;\; \text{where}\;\; w_k = 2c_k, \; b_k=-c_k^{\text{T}}c_k
$$



ì‚¬ì‹¤ ProtoNetì´ ë‹¤ë¥¸ distance metric ë§ê³  euclidean distanceë¥¼ ê³ ë¥¸ ì´ìœ ê°€ ì—¬ê¸°ì— ìˆìŠµë‹ˆë‹¤. Deep Learningì˜ ê¸°ë³¸ conceptì€ backbone networkê°€ feature representationì„ ì›í™œí•˜ê²Œë§Œ í•œë‹¤ë©´ ë‚˜ë¨¸ì§€ëŠ” ìƒí™©ì— ë§ê²Œ (íŠ¹íˆ classification task) linear transformationë§Œ í•´ì£¼ë©´ ë©ë‹ˆë‹¤. ë³´í†µ ì´ëŸ° ê³¼ì •ì„ deep learningì—ì„œëŠ” learnableí•œ linear layerë¥¼ backbone network ë’¤ì— ë¶™ì—¬ì„œ í•™ìŠµì„ í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ProtoNetì€ **<mark>euclidean distanceë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ì´ëŸ° linear transformation ê³¼ì •ì„ ë‚´í¬</mark>**í•˜ê³  ìˆë‹¤ê³  í•´ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, euclidean distanceê°€ ì ì ˆí•˜ë‹¤ê³  ê°€ì •ì„ í•  ìˆ˜ ìˆëŠ” ì´ìœ  ì¤‘ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” í•™ìŠµ ì‹œ non-linearityê°€ í•„ìš”í•œ ë¶€ë¶„ì€ ì´ë¯¸ backbone networkë¥¼ í†µí•´ì„œ ë‹¤ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì´ë¼ê³  ì£¼ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ì´ëŸ°.... assumeì´ ê¼­ í•„ìš”í•œ ë¶€ë¶„ì¸ì§€ëŠ” ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤...)

(ì§„ì§œ ë§ˆì§€ë§‰ìœ¼ë¡œ...) ì´ë ‡ê²Œ reinterpret í–ˆì„ ë•Œ ë˜ ë‹¤ë¥¸ ì¥ì ì€ ProtoNetì— MAML (ì´í•˜ Proto-MAML)ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. $w_k^{\text{T}}f_\theta(x) +b_k$â€‹ ê°€ linear layer ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì— FTê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤. Proto-MAML í•™ìŠµ processì— ëŒ€í•´ (ì§§ê²Œ) ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

- Notation:
  - $f_\theta$: backbone network
  - $g_\theta(x_i) = w_{i,k}^{\text{T}}f_\theta(x_i) +b_k$ 
  - Loss $\mathcal{L}(\mathcal{D};\theta) = \frac{1}{\|\mathcal{D}\|}\sum\_{(x,y)\in\mathcal{D}}l(g_\theta(x), y)$

1. **Inner Loop** 
   1. support set $\mathcal{S}\_i$ ì„ í†µí•´ prototype $c\_{i,k}$ êµ¬í•˜ê¸°  
   2. $\phi  = \theta - \alpha \nabla_\theta \mathcal{L}(\mathcal{S}\_i;\theta)$ 
   3. $n$ step ë°˜ë³µí•˜ê¸° 
2. **Outer Loop**: $\theta \leftarrow \theta - \frac{\beta}{\mathcal{B}}\sum\_{i=1}^{\mathcal{B}}\nabla_\theta \mathcal{L}(\mathcal{Q}_i;\phi)$



ì‹¤ì œë¡œ ë…¼ë¬¸ ì¤‘ [(fo-)Proto-MAML](https://arxiv.org/pdf/1903.03096.pdf)ì„ ì œì‹œí•œ ë…¼ë¬¸ì´ ìˆìŠµë‹ˆë‹¤. ì½ì–´ë³´ì‹œë©´ ë…¼ë¬¸ìƒì—ì„œ ì•„ì£¼ main conceptì€ ì•„ë‹ˆì§€ë§Œ Proto-MAMLì´ ì„±ëŠ¥í–¥ìƒì´ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤.



### *ë§ˆì¹˜ë©°

ì´ ê¸€ê¹Œì§€ meta-learningì˜ ì‹œì´ˆê°€ ë˜ëŠ” ë…¼ë¬¸ë“¤ì„ ê±°ì˜ ë‹¤ ë‹¤ë£¬ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì„¤ëª…í•œ ìœ„ ë…¼ë¬¸ë“¤ì´ ë‚˜ì˜¨ 2016ë…„,  2017ë…„ ì´í›„ì— í•œ 4-5ë…„ê°„ í­ë°œì ìœ¼ë¡œ meta-learning ì—°êµ¬ë“¤ì´ ì´ë£¨ì–´ì¡Œê³ , ì§€ê¸ˆì€ ì•½ê°„ ê°ì†Œë˜ê¸´ í–ˆì§€ë§Œ ê·¸ë˜ë„ ê¾¸ì¤€í•˜ê²Œ top-conference ë…¼ë¬¸ë“¤ì— ê²Œì¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì´ì œëŠ” meta-learningì´ë¼ëŠ” algorithm ìì²´ë¥¼ ì—°êµ¬í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¤ë¥¸ ì—°êµ¬ë“¤ì— ì ‘ëª©ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ë°”ë€ ì¶”ì„¸ì…ë‹ˆë‹¤. íŠ¹íˆ, foundation model ì—°êµ¬ê°€ êµ‰ì¥íˆ í™œë°œí•´ì§€ë©´ì„œ, ì†Œìˆ˜ dataë¡œ í•™ìŠµì„ í•  ìˆ˜ ìˆëŠ” few-shot ê°œë…ì´ ë”ìš± ì¤‘ìš”í•´ì§„ ê²ƒ ê°™ë„¤ì—¬... (AIë¶„ì•¼ì˜ ì„±ì¥ì†ë„ê°€ ë„ˆ~~ë¬´ ë¹ ë¥´ë„¤ì—¬.... ğŸ˜‚ğŸ¥²

ë‹¤ìŒ ì£¼ì œë¡œëŠ” ì œê°€ ì´ì œ ë§‰ ì‹œì‘í•œ foundation model (LLM, LVM ë“±)ì— ëŒ€í•´ì„œ ë…¼ë¬¸ë¦¬ë·°ë“ , ì»¨ì…‰ì´ë“  postingí•´ë³¼ê¹Œ ìƒê°í•©ë‹ˆë‹¤. ì„¸ë¶€ ì£¼ì œë¥¼ ì–´ë”” ìª½ìœ¼ë¡œ ì¡ì„ì§€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì–´ëŠì •ë„ ê³µë¶€ê°€ ëœ í›„ ë‹¤ì‹œ ì°¾ì•„ì˜¤ê² ìŠµë‹ˆë‹¤. ë¶€ì¡±í•œ ê¸€ ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ë§¤ìš°ë§¤ìš° ê°ì‚¬í•©ë‹ˆë‹¹!! :) ğŸ˜ğŸ«¡