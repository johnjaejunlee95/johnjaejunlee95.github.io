---
layout: post
gh-repo: johnjaejunlee95/johnjaejunlee95.github.io
gh-badge: [star, follow]
comments: true
author: johnjaejunlee95
title: "[ë…¼ë¬¸ë¦¬ë·°] Denoising Diffusion Probabilistic Model"
date: "2023-07-21"
permalink: /ddpm/
description: ""
categories: [Generative Model]
toc: False
hits: true
# toc_sticky: True
tags: [Generative, Diffusion Model, Gaussian Distribution]
use_math: true
author_profile: true
sidebar:
  nav: "docs"
---

<div>í˜„ì¬ ì €ëŠ” ëŒ€ë¶€ë¶„ Meta-Learningì— ëŒ€í•œ ì´ë¡  ë° ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œ ê³µë¶€í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ core AI ìª½ìœ¼ë¡œ ê³µë¶€ë¥¼ í•˜ë‹¤ ë³´ë‹ˆ AI ìª½ íŠ¸ëœë“œê°€ êµ‰ì¥íˆ ë¹ ë¥´ê²Œ ë°”ë€Œê³  ìˆìŒì—ë„ ë¹ ë¥´ê²Œ ë”°ë¼ê°€ì§„ ëª»í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì¼ë¶€ ì‹œê°„ì„ í• ì• í•´ì„œ applicableí•œ AIë„ catch up í•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤.  
<br><br>ê·¸ë ‡ê²Œ ë§ˆìŒë¨¹ê³  ì„ íƒí•œ ì²«ë²ˆì§¸ fieldëŠ” Generative Model ìª½ì…ë‹ˆë‹¤. Generative Modelì—ëŠ” VAEë¶€í„° ì‹œì‘í•´ì„œ GAN ë“±ë“± ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ì œê°€ ì…ë¬¸í•œ ì²«ë²ˆì§¸ëŠ” ìµœê·¼ì— ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” Diffusion Modelì…ë‹ˆë‹¤. ê·¸ ì¤‘ì—ì„œë„ ì‹œì´ˆê²©ì¸ ë…¼ë¬¸: Denoising Diffusion Probabilistic Modelë¥¼ ê³¨ëìŠµë‹ˆë‹¤. ğŸ˜ƒ </div>



# Denoising Diffusion Probabilistic Model

## (ê°„ëµí•œ) Generative Modelë“¤ review

![img](/images/23-07-21/generative_model.png)

- GAN: Discriminator ì™€ Generatorë¥¼ ì„œë¡œ ê²½ìŸì‹œì¼œ(adversarial) í•™ìŠµ 
- VAE: Inputì„ Latent space $Z$ì— mapping ì‹œí‚¨ í›„(encoding), latent spaceë¡œë¶€í„° ë‹¤ì‹œ ë³µì›(Decoding)
- Flow-based Models: $f(x)$ í•¨ìˆ˜ë¥¼ í†µí•´ inputì„ space $Z$ ì— mapping ì‹œí‚¨ í›„ $f(x)$ë¥¼ inverseì‹œì¼œ ë³µì›  (iterative)
- Diffusion Model: Gaussian Noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ high entropy ìƒíƒœì—ì„œ ê³¼ì •ì„ ë‹¤ì‹œ reverseí•¨ì„ í†µí•´ ë³µì› (iterative)

**Generative Modelì˜ ìµœì¢… ëª©ì :** \\
$\Rightarrow$ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜ëœ distribution (e.g., Gaussian Distribution)ìœ¼ë¡œë¶€í„°, ì¦‰ ì •ì˜í•˜ê¸° ì‰¬ìš´ distributionìœ¼ë¡œë¶€í„° íŠ¹ì • patternì„ ê°–ëŠ” distribution í˜•íƒœë¡œ mapping

## Denoising Diffusion (Probabilistic) Model

### Diffusion Model Method:

- ê¸°ë³¸ì ìœ¼ë¡œ parameterized Markov Chain í˜•íƒœì˜ model í˜•íƒœë¡œ ì¡´ì¬ (Sequential í•œ í˜•íƒœ)
  ![img](/images/23-07-21/diffusion_model_process.png)
- ì›ë³¸ data $X_0$ ì— ë§¤ stepë§ˆë‹¤ Gaussian Noiseë¥¼ ì¶”ê°€í•˜ì—¬ high entropy ìƒíƒœì˜ data $X_T$ ë¡œ ë³€í™˜ (diffusion; forward process)  
$\rightarrow$ $T = \infty $ ì¼ ê²½ìš° data $X_T$ ëŠ” data $X_0$ ì˜ íŠ¹ì§•(feature)ë¥¼ ì™„ì „íˆ ìƒìŒ
- High Entropy ìƒíƒœì˜ data $X_T$ ë¥¼ ì›ë³¸ data $X_0$ë¡œ ë³µì› (Reverse Process)

### Diffusion Model Process:

**Forward Process** :  
$$q(X_{1:T} | X_0) = \prod_{t=1}^{T} q(X_t |x_{t-1} ), \;  q(X_t | X_{t-1}) = \mathcal{N}(X_t ; \sqrt{1-\beta_t}X_{t-1} , \beta_t I) \tag{1}$$

**Reverse Process** : 
$$p_{\theta}(X_{0:T}) = p(X_T) \prod_{t=1}^T p_{\theta}(X_{t-1}|X_T), \; p_\theta(X_{t-1}|X_t) = \mathcal{N}(X_{t-1} ; \mu_\theta (X_t, t) , \Sigma_\theta(X_t, t)) \tag{2}$$

* $p$ëŠ” í•™ìŠµì‹œì¼œì•¼ í•˜ëŠ” model, $q$ëŠ” fixed (Gaussian Noise)  
$\rightarrow$ ê¸°ì¡´ diffusion modelì—ì„œëŠ” $\beta_t$ë„ í•™ìŠµì„ í†µí•´ ë„ì¶œí–ˆìœ¼ë‚˜, experimentally í™•ì¸ ê²°ê³¼ fixed valueì—¬ë„ í° ì°¨ì´ê°€ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.
* $\beta_t$ ê°€ ì‘ìœ¼ë©´ ì‘ì„ìˆ˜ë¡ $X_t$ì˜ ë³€í™”ìœ¨ $\downarrow$  
$\Rightarrow$ Gaussian Noiseì˜ variance ê°’ì´ $\downarrow$ & mean ê°’ì˜ ë³€í™”ìœ¨ $\downarrow$ 
* Reverse Processì—ì„œ $p_\theta(X_{t-1}|X_T)$ 
$\Rightarrow$ í•™ìŠµì„ í†µí•´  mean & varianceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ update
* **Forward Processì˜ $q$ê°€ Gaussianì„ ë”°ë¥´ê¸° ë•Œë¬¸ì— Reverse Processì—ì„œ $p_\theta$ë„ Gaussianì„ ë”°ë¦…ë‹ˆë‹¤.**
  **$\Rightarrow$  ì¦ëª…: [Feller, 1949](https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/On-the-Theory-of-Stochastic-Processes-with-Particular-Reference-to/chapter/On-the-Theory-of-Stochastic-Processes-with-Particular-Reference-to/bsmsp/1166219215)**

### Diffusion Loss

##### **VAE Loss:** 

$$
\color{red}{D_{KL} (q_\phi (z|x)||p_\theta(z))} \color{black}+\color{blue}{\mathbb{E}_{q_\phi (z|x)}[\log p_\theta (z|x)]} \tag{3} \Rightarrow \color{red}{\text{Regularization}}Â \color{black}+Â \color{blue}{\text{Reconstruction}}
$$



##### Diffusion Loss:



$$\begin{align}
&\color{red}{\mathbb{E}_q [D_{KL}(q(X_T|X_0)||p(X_T))} \color{black}{+} 
 \color{blue}{\sum_{t>1} D_{KL}(q(X_{t-1}|X_t , X_0)||p_{\theta} (X_{t-1}|X_t)) - \log p_\theta(X_0 | X_1)]} \tag{4}
\end{align} $$

- <span style="color:red"> Regularization </span>: learning $\beta_t$
- <span style="color:blue"> Reconstruction </span>: Noise ì œê±° ë° ê¸°ì¡´ data ë³µì›
  - $q (X\_{t-1} \| X\_t , X\_0)$ : Diffusion Processì˜ reverse process ì„¤ëª…

    $$q(X_{t-1}|X_t , X_0) = \mathcal{N}(X_{t-1}; \tilde{\mu}(X_{t-1}, X_0), \beta_t I)) \tag{4-1}$$ 

  - $X_0$ ì˜ ì—­í• : ì´ë¯¸ $X_0$ë¥¼ ì•Œê³  ìˆìœ¼ë‹ˆ $q(X_t\|X_{t-1})$ ë¥¼ <span style="color:Green"><b>BayesianÂ ìœ¼ë¡œ í’€ë©´ $X_{t-1} \leftarrow X_t$ </b></span> ì •ì˜ ê°€ëŠ¥

    $$\tilde{\mu}(X_{t-1}|X_0) = \frac{\sqrt{\tilde{\alpha}_{t-1}} \cdot \beta_t}{1-\tilde{\alpha}_t}X_0 +\frac{\sqrt{\tilde{\alpha}_{t}} \cdot (1-\tilde{\alpha}_{t-1})}{1-\tilde{\alpha}_t}X_t \tag{5}$$

    where  $\tilde{\beta}\_t = \frac{1-\tilde{\alpha}\_{t-1}} {1 - \tilde{\alpha}\_t} \beta_t$ , $\alpha_t = 1- \beta_t$ , $\tilde{\alpha}\_t = \prod\_{s=1}^{t} \alpha_t $
  - $p_\theta (X_{t-1} \| X_t)$ : ì‹¤ì œ noiseë¥¼ ë³´ê³  reverse process ì˜ˆì¸¡(í•™ìŠµ)

***Bayes Rule:** <span style="color:Green">$q(X_{t-1}|X_t,X_0) = q(X_t|X_{t-1}, X_0) \frac{q(X_{t-1}|X_t)}{q(X_t | X_0)}$</span>

***Reparameterization:**  

$$\begin{aligned} 
X_t &= \sqrt{\alpha_t}X_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1}\\ &= \sqrt{\alpha_t \alpha_{t-1}}X_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}}\bar{\epsilon}_{t-2} \\ &=\cdots \\ &= \sqrt{\bar{\alpha_t}}X_0 + \sqrt{1-\bar{\alpha}_t}\epsilon 
\end{aligned} \tag{6}$$



where $\epsilon_{t-1} = \mathcal{N}(0, I)$ , $\bar{\epsilon}\_{t-t'} = \mathcal{N} (0, (\sum_{t=1}^{t'} \sigma^2_{t'}) I)\;$  and $\; \bar{\alpha}\_t = \prod\_{t=1}^t \alpha_i$

ì—¬ê¸°ì„œ $X_t$ë¥¼ $X_0$ ì— ëŒ€í•œ ì‹ìœ¼ë¡œ ì¹˜í™˜í•˜ë©´

$$X_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(X_t - \sqrt{1-\bar{\alpha}_t}\epsilon_t)\tag{7}$$



$X_t$ì™€ $X_0$ë¥¼ $(5)$ ì‹ì— ëŒ€ì…í•˜ë©´  

$$\tilde{\mu}_t (X_t, X_0) = \frac{1}{\sqrt{\alpha_t}}(X_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t) \tag{8}$$



$\therefore$ $(6)$ì„ í™œìš©í•˜ì—¬ reparameterization ì§„í–‰ í›„ $\tilde{\mu}_t$ë¥¼ $X_t$ ì™€ $\epsilon_t$ ì‹ìœ¼ë¡œ ì¹˜í™˜ ê°€ëŠ¥


### Denoising Diffusion Probabilistic Model (DDPM)

**Loss Simplification: Only <span style="color:blue"> Reconstruction </span>term ($\Rightarrow$ Inductive Bias $\uparrow$)**

- Use fixed $\beta_t$ (not to learn $\beta_t$) $\rightarrow$ Delete<span style="color:red"> Regularization </span>term

- Not to learn Variance  
  $\rightarrow$ $\Sigma\_{\theta} (X_t, t) = \sigma^2\_t I$ ;  $\sigma^2\_t = \tilde{\beta}\_t = \frac{1-\tilde{\alpha}\_{t-1}} {1-\tilde{\alpha}\_t} \beta_t $ or  $\sigma^2_t = \beta_t$ 

- Train $\mu_\theta(X_t , t) \rightarrow$  $X_{t-1}$ ì˜ gaussian meanì„ ì˜ˆì¸¡, ì¦‰ $t$ ì‹œì ì—ì„œ $t-1$ ì‹œì ì¼ ë•Œì˜ meanì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ $\mu_\theta$ë¥¼ í•™ìŠµ
  $\Rightarrow p_\theta(X_{t-1}|X_t) = \mathcal{N}(X_{t-1} ; \mu_\theta (X_t, t) , \Sigma_\theta(X_t, t))$ ì„ í•™ìŠµí•œë‹¤ê³  í–ˆì„ ë•Œ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒ ì²˜ëŸ¼ variance($\Sigma_\theta$)ëŠ” ê³ ì •ì„ í•˜ê³  gaussian distributionì„ ë”°ë¥´ë¯€ë¡œ meanì— í•´ë‹¹í•˜ëŠ” $\mu_\theta $ë§Œ estimate í•˜ë©´ ë¨

ë‹¤ì‹œ ì •ë¦¬í•˜ë©´, 

- Diffusion Lossì—ì„œì˜ reconstruction termì„ ë³´ë©´ ìµœì¢…ì ìœ¼ë¡œ parameterize ë° train ì‹œì¼œì•¼ í•˜ëŠ” variableì€ $p_\theta$

- $p_\theta$ë¥¼ í•™ìŠµ ì‹œ í•´ë‹¹ variableì´ gaussian distributionì„ ë”°ë¥´ë¯€ë¡œ mean and varianceì„ í•™ìŠµì‹œí‚¤ë©´ ë¨

- ë‹¤ë§Œ, DDPMì—ì„œëŠ” varianceë¥¼ fixed termìœ¼ë¡œ ë‘ê³  í•™ìŠµì„ í•˜ë¯€ë¡œ $\mu_\theta$ë§Œ í•™ìŠµì‹œí‚¤ë©´ ë¨

- Diffusion Modelì—ì„œëŠ” $(4-1), (5), (8)$ ì‹ë“¤ì„ ì ìš©í•´ $\tilde{\mu_t}$ë¥¼ estimate í•˜ë©´ ë¨ 
  $\rightarrow$ $\tilde{\mu}_t$ë¥¼ $\mu\_{\theta}$ ë¡œ ì¹˜í™˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ

  $$\mu_\theta(X_t, t) = \frac{1}{\sqrt{\alpha_t}} (X_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta (X_t, t) ) \tag{9}$$
  
  $\Rightarrow \epsilon_\theta (X_t, t)$ëŠ” t ì‹œì ì—ì„œì˜ parameterized gaussian distribution (noise) 


Diffusion Modelì—ì„œì˜ <span style="color:blue">Â reconstruction loss</span> termë§Œ ë³´ë©´ ê²°êµ­ <span style="color:blue">$q(X_{t-1}|X_t , X_0)$ & $p_\theta (X_{t-1}|X_t)$</span> ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ìª½ìœ¼ë¡œ í•™ìŠµ ì§„í–‰ 

$\Rightarrow$ ì—¬ê¸°ì„œ $q$ ì™€ $p_\theta$ ëª¨ë‘ gaussian distributionì´ê³  varianceëŠ” fixed valueì´ë¯€ë¡œ ì„œë¡œê°„ì˜ $\mu$ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ìª½ìœ¼ë¡œ í•™ìŠµ ì§„í–‰

$\Rightarrow$ ì¦‰, $\mathbb{E}[(8) - (9)]$ ìœ¼ë¡œ ì‹ì„ ì¹˜í™˜í•´ë„ ë¬´ë°©

$\therefore$ ì´ë¥¼ ì „ê°œ ë° simplify í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ë„ì¶œ

$$\begin{align}
L_{t-1} &= \mathbb{E}_q[\frac{1}{2\sigma^2_t}||\tilde{\mu}_t (X_t, X_0) - \mu_\theta (X_t, t)||^2] + C\\
\Rightarrow L_{simple}(\theta)&=  \mathbb{E}_{t, x_0, \epsilon} [||\epsilon_t - \epsilon_\theta (\sqrt{\tilde{\alpha}_t}X_0 + \sqrt{1-\tilde{\alpha}_t}\epsilon , t )||^2] \tag{10}
\end{align}$$

(ì´ ê³¼ì •ì´ denoising ê³¼ì •ì´ê¸° ë–„ë¬¸ì— ë…¼ë¬¸ì—ì„œ denoisingì´ ë¶™ìŒ)


## Conclusion

ì‚¬ì‹¤ìƒ ê±°ì˜ ì²˜ìŒìœ¼ë¡œ ì œëŒ€ë¡œ ì½ì–´ë³¸ ìƒì„±ëª¨ë¸ì´ì˜€ëŠ”ë° ìƒë‹¹íˆ í¥ë¯¸ë¡œì› ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¨ìˆœíˆ í•™ìŠµì— ë§¡ê¸°ëŠ” ê²ƒì´ ì•„ë‹Œ ìˆ˜í•™ì ìœ¼ë¡œ ì˜ ì •ì˜í•¨ê³¼ ë™ì‹œì— ì´ ì˜ ì •ì˜ëœ ìˆ˜í•™ì„ ê³µí•™ì ìœ¼ë¡œ ì˜ í™œìš©í–ˆë‹¤ëŠ” ëŠë‚Œì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì‹¤ì€ ìˆœìˆ˜ ìˆ˜í•™ì„ ì´ ì„¸ìƒ ì†ì— ì ìš©í•˜ê¸°ëŠ” ë§¤ìš° ì–´ë µì§€ë§Œ ì´ë¥¼ approximate, ì¦‰ ê³µí•™ì ìœ¼ë¡œ í’€ì–´ëƒˆì„ ë•Œ ìš°ë¦¬ì—ê²Œ êµ‰ì¥íˆ ì‹¤ìš©ì ì´ê²Œ ë˜ê³ , ì´ Diffusion Model, DDPM ë“±ì˜ ë…¼ë¬¸ì´ ì˜ ë³´ì—¬ì¤€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œëŠ” DDPM ë‹¤ìŒìœ¼ë¡œ ë‚˜ì˜¨ <span style="color:Orange">Denoising Diffusion Implicit Model (DDIM)</span> ë…¼ë¬¸ì„ ë¦¬ë·°í•´ë³¼ê¹Œ í•©ë‹ˆë‹¤. ë§¤ìš° ë¶€ì¡±í•˜ì§€ë§Œ ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!!

## Reference:

- [Denoising Diffusion Probabilistic Model(DDPM) Paper](https://arxiv.org/pdf/2006.11239.pdf)
- [Diffusion Model ìˆ˜í•™ì´ í¬í•¨ëœ tutorial](https://youtu.be/uFoGaIVHfoE)
- [Lil'Log's blog - What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [PR-409 Denoising Diffusion Probabilistic Models](https://www.youtube.com/watch?v=1j0W_lu55nc)

